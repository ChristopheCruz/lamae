# LAMAe 

L'objectif du projet de Laboratoire commun LAMAe est de développer des savoirs dans le domaine des Agents Multimodaux Autonomes et Empathiques.
Un "humain digital autonome" désigne une représentation virtuelle d'un individu, utilisée dans divers contextes numériques tels que la réalité virtuelle, les jeux vidéo, les applications de communication, etc. Cette représentation peut prendre la forme d'un avatar, d'un assistant virtuel ou d'un personnage interactif. L’entreprise DAVI partenaire de longue date du porteur de projet et engagé dans cette proposition de laboratoire commun est un acteur majeur sur le marché des agents conversationnels émotionnels. Leurs agents ont la capacité d’interagir avec les utilisateurs en faisant preuve d’empathie rendant les interactions proches des interactions humaines.

Nous visons la réalisation d’humains digitaux capables d'interagir de manière naturelle avec les utilisateurs pour améliorer l'expérience d’interaction sur le long cours en intégrant la dimension relationnelle, à contrario des chatbots développés seulement pour répondre aux questions. Nous visons le développement d’agents avec des capacités de réalisation de séquences de tâches complexes dans une variété de domaines, de répondre à des questions métiers, de former, mais aussi de maintenir ses connaissances métiers à jour, tout en adoptant un comportement proactif. L'accent serait mis sur la conception d'algorithmes avancés de traitement du langage naturel, de vision par ordinateur pour créer des interactions personnalisées et immersives

Ce repo inclut une liste de papiers sur les AI agentiques et empathiques.

## Etat de l'art 



<!--
## Table of Contents

- [SLM Survey](#slm-survey)
  - [Table of Contents](#table-of-contents)
  - [Overview of SLMs](#overview-of-slms)
  - [Timeline of SLMs](#timeline-of-slms)
  - [SLMs Paper List](#slms-paper-list)
    - [Existing SLMs](#existing-slms)
    - [Foundational Concepts in Building Language Models](#foundational-concepts-in-building-language-models)
    - [Advanced enhancement methods for SLM](#advanced-enhancement-methods-for-slm)
      - [Training from scratch](#training-from-scratch)
      - [Supervised fine-tuning](#supervised-fine-tuning)
      - [Data quality in KD](#data-quality-in-kd)
      - [Distillation for SLM](#distillation-for-slm)
      - [Quantization](#quantization)
      - [LLMs for SLM](#llms-for-slm)
    - [Task-specific SLM Applications](#task-specific-slm-applications)
      - [SLM in QA](#slm-in-qa)
      - [SLM in Coding](#slm-in-coding)
      - [SLM in Recommendation](#slm-in-recommendation)
      - [SLM in Web Search](#slm-in-web-search)
      - [SLM in Mobile-device](#slm-in-mobile-device)
    - [On-device Deployment Optimization Techniques](#on-device-deployment-optimization-techniques) 
      - [Memory Efficiency Optimization](#memory-efficiency-optimization)
      - [Runtime Efficiency Optimization](#runtime-efficiency-optimization)
-->
